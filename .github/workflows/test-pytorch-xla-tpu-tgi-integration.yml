name: Optimum TPU / Test TGI on TPU / Integration Tests

on:
  push:
  pull_request:
    branches: [ main ]
    paths:
      - "text-generation-inference/**"
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  integration-tests:
    name: Run TGI Integration Tests
    runs-on:
      group: gcp-ct5lp-hightpu-8t

    # the error (http://metadata.google.internal/computeMetadata/v1 not accesible ) is the 
    # container:
    #     image: us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:r2.4.0_3.10_tpuvm
    #     options: --shm-size "16gb" --ipc host --privileged ${{ vars.V5_LITEPOD_8_ENV}} -v /mnt/hf_cache:/mnt/hf_cache

    env:
      PJRT_DEVICE: TPU
      HF_HUB_CACHE: /mnt/hf_cache/cache_huggingface
      HF_TOKEN: ${{ secrets.HF_TOKEN_OPTIMUM_TPU_CI }}
      V5_LITEPOD_8_ENV: ${{ vars.V5_LITEPOD_8_ENV}}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # - name: Install dependencies
      #   run: |
      #     apt-get update -y
      #     apt-get install -y python3 python3-pip python-is-python3

      - name: Install dependencies (container less)
        run: |
          sudo apt-get update -y
          sudo apt-get install -y python3 python3-pip python-is-python3

      # - name: Install dependencies + Build TGI server
      #   run: |
      #     make tgi_docker_integration_test_installs

      # - name: Run integration tests
      #   run: |
      #     make tgi_docker_integration_test

      # this is just a debug step normally tpu should have access to http://metadata.google.internal/computeMetadata/v1
      # - name: Debug GCP metadata
      #   run: |
      #     python -m pip install requests
      #     python debugging.py

      - name: Debug XLA
        run: |
          python -m pip install torch~=2.5.0 torch_xla[tpu]~=2.5.0 -f https://storage.googleapis.com/libtpu-releases/index.html
          python xla_add.py
