# TPU hardware support
Optimum-TPU support and is optimized for V5e, V5p, and V6e TPUs.

## When to use TPU
TPUs excel at large-scale machine learning workloads with matrix computations, extended training periods, and large batch sizes. In contrast, GPUs offer more flexibility for models with custom operations or mixed CPU/GPU workloads. TPUs aren't ideal for workloads needing frequent branching, high-precision arithmetic, or custom training loop operations. More information can be found at https://cloud.google.com/tpu/docs/intro-to-tpu#when_to_use_tpus

## TPU naming convention
The TPU naming follows this format: `<tpu_version>-<number_of_tpus>`

TPU versions available: 
- v5litepod (v5e)
- v5p 
- v6e.

For example, a v5litepod-8 is a v5e TPU with 8 tpus.

## Memory on TPU
The HBM (High Bandwidth Memory) capacity per chip is 16gb for V5e, V5p and 32gb for V6e. So a v5e-8 (v5litepod-8), has 16gb*8=128gb of HBM memory

## Performance on TPU
There are several key metrics to consider when evaluating TPU performance:
- Peak compute per chip (bf16/int8): Measures the maximum theoretical computing power in floating point or integer operations per second. Higher values indicate faster processing capability for machine learning workloads.
HBM (High Bandwidth Memory) metrics:
- Capacity: Amount of available high-speed memory per chip
- Bandwidth: Speed at which data can be read from or written to memory
These affect how much data can be processed and how quickly it can be accessed.
- Inter-chip interconnect (ICI) bandwidth: Determines how fast TPU chips can communicate with each other, which is crucial for distributed training across multiple chips.
Pod-level metrics:
- Peak compute per Pod: Total computing power when multiple chips work together
These indicate performance at scale for large training or serving jobs.

The actual performance you achieve will depend on your specific workload characteristics and how well it matches these hardware capabilities.

## Recommended Runtime for TPU

During the TPU VM creation use the following TPU VM base images for optimum-tpu:
- v2-alpha-tpuv6e (TPU v6e) (recommended)
- v2-alpha-tpuv5 (TPU v5p) (recommended)
- v2-alpha-tpuv5-lite (TPU v5e) (recommended)
- tpu-ubuntu2204-base (default)

For installation instructions, refer to our [TPU setup tutorial](./tutorials/tpu_setup). We recommend you use the *alpha* version with optimum-tpu, as optimum-tpu is tested and optimized for those.

More information at https://cloud.google.com/tpu/docs/runtimes#pytorch_and_jax

# Next steps
For more information on the different TPU hardware, you can look at:
https://cloud.google.com/tpu/docs/v6e
https://cloud.google.com/tpu/docs/v5p
https://cloud.google.com/tpu/docs/v5e

Pricing informatin can be found here https://cloud.google.com/tpu/pricing

Tpu availability can be found https://cloud.google.com/tpu/docs/regions-zones