# Deploying a Text-Generation Inference server on a Google Cloud TPU instance

Serving Large Language Models (LLMs) on Optimum-TPU is mainly done via TGI. TGI is a highly optimized serving engine that enables LLMs to be served in the most performant way. At Optimum-TPU, we have optimzied TGI for the TPU hardware. TGI also offers an openAI-compatible API, making it easy to integrate with numerous tools.

## Supported Models

See [Supported Models](../supported-architectures).

## Deploy TGI on Cloud TPU Instance Locally

This guide assumes you have a Cloud TPU instance running. If not, please refer to our [deployment guide](./deploy).

You have two options for deploying TGI:
1. Use our pre-built TGI image (recommended)
2. Build the image manually for the latest features

### Option 1: Using the Pre-built Image

The optimum-tpu image is available at `ghcr.io/huggingface/optimum-tpu:v0.2.3-tgi`. Please look at [optimum-tpu container documentation](../optimum_container) for the latest TGI image. The [tutorial on serving](../tutorials/inference_on_tpu) also walks you through how to start the TGI container from a pre-built image. Here's how to deploy it:

```bash
docker run -p 8080:80 \
        --shm-size 16G \
        --privileged \
        --net host \
        -e LOG_LEVEL=text_generation_router=debug \
        -v ~/hf_data:/data \
        -e HF_TOKEN=$(cat ~/.cache/huggingface/token) \
        -e SKIP_WARMUP=1 \
        ghcr.io/huggingface/optimum-tpu:v0.2.3-tgi \
        --model-id google/gemma-2b-it \
        --max-input-length 512 \
        --max-total-tokens 1024 \
        --max-batch-prefill-tokens 512 \
        --max-batch-total-tokens 1024
```

You can also use the GCP-provided image as referenced in the [optimum-tpu container page](../optimum_container)

### Option 2: Manual Image Building

For the latest features (main branch of optimum-tpu) or custom modifications, build the image yourself:

1. Clone the repository:
```bash
git clone https://github.com/huggingface/optimum-tpu.git
```

2. Build the image:
```bash
make tpu-tgi
```

3. Run the container:
```bash
HF_TOKEN=<your_hf_token_here>
MODEL_ID=google/gemma-2b-it

sudo docker run --net=host \
                --privileged \
                -v $(pwd)/data:/data \
                -e HF_TOKEN=${HF_TOKEN} \
                huggingface/optimum-tpu:latest \
                --model-id ${MODEL_ID} \
                --max-concurrent-requests 4 \
                --max-input-length 32 \
                --max-total-tokens 64 \
                --max-batch-size 1
```

## Executing requests against the service

You can query the model using either the `/generate` or `/generate_stream` routes:

bash
```
curl localhost/generate \
    -X POST \
    -d '{"inputs":"What is Deep Learning?","parameters":{"max_new_tokens":20}}' \
    -H 'Content-Type: application/json'
```

bash
```
curl localhost/generate_stream \
    -X POST \
    -d '{"inputs":"What is Deep Learning?","parameters":{"max_new_tokens":20}}' \
    -H 'Content-Type: application/json'
```