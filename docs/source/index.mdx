<!---
Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# ðŸ¤— Optimum TPU

<Tip>
**ðŸš§ Optimum-TPU is now in maintenance mode.**

Weâ€™ll continue to welcome community contributions for minor bug fixes, documentation improvements, and lightweight maintenance tasks.

While this project is no longer under active development, you can continue exploring TPU solutions with:

- [TPU inference](./tutorials/inference_on_tpu) for inference
- [ðŸ¤— Accelerate](https://github.com/huggingface/accelerate) for training


</Tip>

Optimum TPU provides all the necessary machinery to leverage and optimize AI workloads running on [Google Cloud TPU devices](https://cloud.google.com/tpu/docs). Optimum-TPU is a HuggingFace solution to optimize HuggingFace products for the TPU platform. This allows users to use HuggingFace features and easy-to-use libraries on TPU with the best performance. We currently optimize transformers and TGI and integrate [HuggingFace hub](https://huggingface.co/models) so you can access HuggingFace's large library of models.

If you are here to start using HuggingFace products on TPU, then you are in the right place

The API provides the overall same user-experience as HuggingFace transformers with the minimum amount of changes required to target performance for inference and training.

Optimum TPU is meant to reduce as much as possible the friction in order to leverage Google Cloud TPU accelerators.
As such, we provide a pip installable package to make sure everyone can get easily started.

```bash
pip install optimum-tpu -f https://storage.googleapis.com/libtpu-releases/index.html
```

## Why Choose TPUs
TPUs excel at large-scale machine learning workloads with matrix computations, extended training periods, and large batch sizes. In contrast, GPUs offer more flexibility for models with custom operations or mixed CPU/GPU workloads. TPUs aren't ideal for workloads needing frequent branching, high-precision arithmetic, or custom training loop operations. More information can be found at https://cloud.google.com/tpu/docs/intro-to-tpu#when_to_use_tpus

## Why Choose Optimum-TPU
Optimum-TPU serves as the bridge between the HuggingFace ecosystem and Google Cloud TPU hardware. It dramatically simplifies what would otherwise be a complex integration process, providing an intuitive interface that abstracts away TPU-specific implementation details while maintaining high performance. Through automated optimizations, efficient batching strategies, intelligent memory management and more, Optimum-TPU ensures your models run at peak efficiency on TPU hardware. The framework's deep integration with the HuggingFace Hub catalog of models and datasets enables easy deployment and fine-tuning of state-of-the-art models with the familiar ease of use of HuggingFace libraries while maximizing TPU hardware capabilities.

<div class="mt-10">
    <div class="w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 lg:grid-cols-2 md:gap-y-4 md:gap-x-5">
        <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./tutorials/tpu_setup">
            <div class="w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">
                Tutorials
            </div>
            <p class="text-gray-700">
                Learn the basics and become familiar with deploying transformers on Google TPUs.
                Start here if you are using ðŸ¤— Optimum-TPU for the first time!
            </p>
        </a>
        <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./howto/serving">
            <div class="w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">
                How-to guides
            </div>
            <p class="text-gray-700">
                Practical guides to help you achieve a specific goal. Take a look at these guides to learn how to use ðŸ¤— Optimum-TPU
                to solve real-world problems.
            </p>
        </a>
        <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./conceptual_guides/tpu_hardware_support">
            <div class="w-full text-center bg-gradient-to-br from-green-400 to-green-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">
                Conceptual Guides
            </div>
            <p class="text-gray-700">
                Deep dives into key concepts behind TPU optimization, architecture, and best practices.
                Understand how TPUs work and how to maximize their potential.
            </p>
        </a>
        <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./reference/fsdp_v2">
            <div class="w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">
                Reference
            </div>
            <p class="text-gray-700">
                Technical descriptions of how the classes and methods of ðŸ¤— Optimum-TPU work.
                Detailed API documentation, configuration options, and implementation details.
            </p>
        </a>
    </div>
</div>